{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvMvB43q64Yx",
        "outputId": "fa91943c-da2c-4b0f-955f-58a71430f76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/gdrive')\n",
        "os.chdir(\"gdrive/MyDrive/Yin/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqXfLrKP78lP",
        "outputId": "d49df345-d345-48ac-b8fb-2782717297f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dflLSPXG76rS"
      },
      "outputs": [],
      "source": [
        "!pip install ludwig[text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhPhq8DCLECS"
      },
      "outputs": [],
      "source": [
        "!pip install ludwig==0.4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DVLbaIm8d2Z",
        "outputId": "60f2f523-f00d-4f13-d3e7-dd00c16426d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.4.1 - Train\n",
            "\n",
            "Experiment name: experiment\n",
            "Model name: run\n",
            "Output directory: results/experiment_run_2\n",
            "\n",
            "\n",
            "ludwig_version: '0.4.1'\n",
            "command: ('/usr/local/bin/ludwig train --dataset '\n",
            " '/content/gdrive/MyDrive/Yin/climate/train/alaska_climate_change_lables.tsv '\n",
            " '--config /content/gdrive/MyDrive/Yin/climate/model/bert.yaml')\n",
            "random_seed: 42\n",
            "dataset: '/content/gdrive/MyDrive/Yin/climate/train/alaska_climate_change_lables.tsv'\n",
            "data_format: 'tsv'\n",
            "config: {   'combiner': {'type': 'concat'},\n",
            "    'input_features': [   {   'column': 'text',\n",
            "                              'encoder': 'bert',\n",
            "                              'level': 'word',\n",
            "                              'name': 'text',\n",
            "                              'preprocessing': {   'padding_symbol': '[PAD]',\n",
            "                                                   'unknown_symbol': '[UNK]',\n",
            "                                                   'word_tokenizer': 'english_tokenize_remove_stopwords'},\n",
            "                              'pretrained_model_name_or_path': 'bert-base-uncased',\n",
            "                              'proc_column': 'text_TeWL7r',\n",
            "                              'tied': None,\n",
            "                              'type': 'text'}],\n",
            "    'output_features': [   {   'column': 'class',\n",
            "                               'dependencies': [],\n",
            "                               'loss': {   'class_similarities_temperature': 0,\n",
            "                                           'class_weights': 1,\n",
            "                                           'confidence_penalty': 0,\n",
            "                                           'labels_smoothing': 0,\n",
            "                                           'robust_lambda': 0,\n",
            "                                           'type': 'softmax_cross_entropy',\n",
            "                                           'weight': 1},\n",
            "                               'name': 'class',\n",
            "                               'proc_column': 'class_mZFLky',\n",
            "                               'reduce_dependencies': 'sum',\n",
            "                               'reduce_input': 'sum',\n",
            "                               'top_k': 3,\n",
            "                               'type': 'category'}],\n",
            "    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\n",
            "                                      'audio_file_length_limit_in_s': 7.5,\n",
            "                                      'in_memory': True,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'norm': None,\n",
            "                                      'padding_value': 0},\n",
            "                         'bag': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'binary': {   'fill_value': 0,\n",
            "                                       'missing_value_strategy': 'fill_with_const'},\n",
            "                         'category': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 10000},\n",
            "                         'date': {   'datetime_format': None,\n",
            "                                     'fill_value': '',\n",
            "                                     'missing_value_strategy': 'fill_with_const'},\n",
            "                         'force_split': False,\n",
            "                         'h3': {   'fill_value': 576495936675512319,\n",
            "                                   'missing_value_strategy': 'fill_with_const'},\n",
            "                         'image': {   'in_memory': True,\n",
            "                                      'infer_image_dimensions': True,\n",
            "                                      'infer_image_max_height': 256,\n",
            "                                      'infer_image_max_width': 256,\n",
            "                                      'infer_image_num_channels': True,\n",
            "                                      'infer_image_sample_size': 100,\n",
            "                                      'missing_value_strategy': 'backfill',\n",
            "                                      'num_processes': 1,\n",
            "                                      'resize_method': 'interpolate',\n",
            "                                      'scaling': 'pixel_normalization'},\n",
            "                         'numerical': {   'fill_value': 0,\n",
            "                                          'missing_value_strategy': 'fill_with_const',\n",
            "                                          'normalization': None},\n",
            "                         'sequence': {   'fill_value': '<UNK>',\n",
            "                                         'lowercase': False,\n",
            "                                         'missing_value_strategy': 'fill_with_const',\n",
            "                                         'most_common': 20000,\n",
            "                                         'padding': 'right',\n",
            "                                         'padding_symbol': '<PAD>',\n",
            "                                         'sequence_length_limit': 256,\n",
            "                                         'tokenizer': 'space',\n",
            "                                         'unknown_symbol': '<UNK>',\n",
            "                                         'vocab_file': None},\n",
            "                         'set': {   'fill_value': '<UNK>',\n",
            "                                    'lowercase': False,\n",
            "                                    'missing_value_strategy': 'fill_with_const',\n",
            "                                    'most_common': 10000,\n",
            "                                    'tokenizer': 'space'},\n",
            "                         'split_probabilities': (0.7, 0.1, 0.2),\n",
            "                         'stratify': None,\n",
            "                         'text': {   'char_most_common': 70,\n",
            "                                     'char_sequence_length_limit': 1024,\n",
            "                                     'char_tokenizer': 'characters',\n",
            "                                     'char_vocab_file': None,\n",
            "                                     'fill_value': '<UNK>',\n",
            "                                     'lowercase': True,\n",
            "                                     'missing_value_strategy': 'fill_with_const',\n",
            "                                     'padding': 'right',\n",
            "                                     'padding_symbol': '<PAD>',\n",
            "                                     'pretrained_model_name_or_path': None,\n",
            "                                     'unknown_symbol': '<UNK>',\n",
            "                                     'word_most_common': 20000,\n",
            "                                     'word_sequence_length_limit': 256,\n",
            "                                     'word_tokenizer': 'space_punct',\n",
            "                                     'word_vocab_file': None},\n",
            "                         'timeseries': {   'fill_value': '',\n",
            "                                           'missing_value_strategy': 'fill_with_const',\n",
            "                                           'padding': 'right',\n",
            "                                           'padding_value': 0,\n",
            "                                           'timeseries_length_limit': 256,\n",
            "                                           'tokenizer': 'space'},\n",
            "                         'vector': {   'fill_value': '',\n",
            "                                       'missing_value_strategy': 'fill_with_const'}},\n",
            "    'training': {   'batch_size': 8,\n",
            "                    'bucketing_field': None,\n",
            "                    'decay': True,\n",
            "                    'decay_rate': 0.96,\n",
            "                    'decay_steps': 10000,\n",
            "                    'early_stop': 5,\n",
            "                    'epochs': 100,\n",
            "                    'eval_batch_size': None,\n",
            "                    'gradient_clipping': None,\n",
            "                    'increase_batch_size_on_plateau': 0,\n",
            "                    'increase_batch_size_on_plateau_max': 512,\n",
            "                    'increase_batch_size_on_plateau_patience': 5,\n",
            "                    'increase_batch_size_on_plateau_rate': 2,\n",
            "                    'learning_rate': 2e-05,\n",
            "                    'learning_rate_warmup_epochs': 1,\n",
            "                    'optimizer': {   'beta_1': 0.9,\n",
            "                                     'beta_2': 0.999,\n",
            "                                     'epsilon': 1e-08,\n",
            "                                     'type': 'adam'},\n",
            "                    'reduce_learning_rate_on_plateau': 0,\n",
            "                    'reduce_learning_rate_on_plateau_patience': 5,\n",
            "                    'reduce_learning_rate_on_plateau_rate': 0.5,\n",
            "                    'regularization_lambda': 0,\n",
            "                    'staircase': False,\n",
            "                    'trainable': True,\n",
            "                    'validation_field': 'combined',\n",
            "                    'validation_metric': 'loss'}}\n",
            "tf_version: '2.7.3'\n",
            "\n",
            "\n",
            "Using full raw dataset, no hdf5 and json file with the same name have been found\n",
            "Building dataset (it may take a while)\n",
            "Downloading: 100% 570/570 [00:00<00:00, 697kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 929kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.49MB/s]\n",
            "Writing preprocessed training set cache\n",
            "Writing preprocessed test set cache\n",
            "Writing preprocessed validation set cache\n",
            "Writing train set metadata\n",
            "Training set: 1554\n",
            "Validation set: 215\n",
            "Test set: 454\n",
            "Downloading: 100% 536M/536M [00:26<00:00, 20.1MB/s]\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "\n",
            "╒══════════╕\n",
            "│ TRAINING │\n",
            "╘══════════╛\n",
            "\n",
            "\n",
            "Epoch   1\n",
            "Training:   0% 0/195 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Training:  99% 194/195 [00:32<00:00,  9.96it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Training: 100% 195/195 [00:37<00:00,  5.14it/s]\n",
            "Evaluation train:   0% 0/195 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Evaluation train:  99% 193/195 [00:07<00:00, 33.30it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Evaluation train: 100% 195/195 [00:08<00:00, 21.82it/s]\n",
            "Evaluation vali :  93% 25/27 [00:00<00:00, 33.86it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Evaluation vali : 100% 27/27 [00:01<00:00, 13.66it/s]\n",
            "Evaluation test :  93% 53/57 [00:01<00:00, 33.04it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Evaluation test : 100% 57/57 [00:02<00:00, 19.70it/s]\n",
            "Took 51.7910s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.2688 │     0.9073 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.3770 │     0.8605 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.4133 │     0.8414 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.2688 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3770 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.4133 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved\n",
            "\n",
            "\n",
            "Epoch   2\n",
            "Training: 100% 195/195 [00:20<00:00,  9.75it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.00it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.25it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.82it/s]\n",
            "Took 28.4640s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.1295 │     0.9633 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.3787 │     0.8837 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.4763 │     0.8348 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.1295 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3787 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.4763 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 1 epoch ago\n",
            "\n",
            "\n",
            "Epoch   3\n",
            "Training: 100% 195/195 [00:20<00:00,  9.64it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.18it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.36it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.51it/s]\n",
            "Took 28.6645s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0574 │     0.9858 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.3369 │     0.8837 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.4666 │     0.8656 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0574 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3369 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.4666 │\n",
            "╘════════════╧════════╛\n",
            "Validation loss on combined improved, model saved\n",
            "\n",
            "\n",
            "Epoch   4\n",
            "Training: 100% 195/195 [00:20<00:00,  9.67it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.34it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.11it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.77it/s]\n",
            "Took 28.5716s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0272 │     0.9923 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.3941 │     0.8977 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.6349 │     0.8524 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0272 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.3941 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.6349 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 1 epoch ago\n",
            "\n",
            "\n",
            "Epoch   5\n",
            "Training: 100% 195/195 [00:19<00:00,  9.76it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.29it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.18it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 34.02it/s]\n",
            "Took 28.3705s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0204 │     0.9955 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.6569 │     0.8930 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.8689 │     0.8480 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0204 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.6569 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.8689 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 2 epochs ago\n",
            "\n",
            "\n",
            "Epoch   6\n",
            "Training: 100% 195/195 [00:19<00:00,  9.78it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.46it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 33.77it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.36it/s]\n",
            "Took 28.3360s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0116 │     0.9974 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.6270 │     0.8884 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.8375 │     0.8546 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0116 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.6270 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.8375 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 3 epochs ago\n",
            "\n",
            "\n",
            "Epoch   7\n",
            "Training: 100% 195/195 [00:20<00:00,  9.74it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.36it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.14it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.76it/s]\n",
            "Took 28.4251s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0235 │     0.9949 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.5320 │     0.9070 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.8311 │     0.8502 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0235 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.5320 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.8311 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 4 epochs ago\n",
            "\n",
            "\n",
            "Epoch   8\n",
            "Training: 100% 195/195 [00:19<00:00,  9.80it/s]\n",
            "Evaluation train: 100% 195/195 [00:05<00:00, 33.15it/s]\n",
            "Evaluation vali : 100% 27/27 [00:00<00:00, 34.26it/s]\n",
            "Evaluation test : 100% 57/57 [00:01<00:00, 33.88it/s]\n",
            "Took 28.3292s\n",
            "╒═════════╤════════╤════════════╤═════════════╕\n",
            "│ class   │   loss │   accuracy │   hits_at_k │\n",
            "╞═════════╪════════╪════════════╪═════════════╡\n",
            "│ train   │ 0.0110 │     0.9968 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ vali    │ 0.5849 │     0.8930 │      1.0000 │\n",
            "├─────────┼────────┼────────────┼─────────────┤\n",
            "│ test    │ 0.9267 │     0.8502 │      1.0000 │\n",
            "╘═════════╧════════╧════════════╧═════════════╛\n",
            "╒════════════╤════════╕\n",
            "│ combined   │   loss │\n",
            "╞════════════╪════════╡\n",
            "│ train      │ 0.0110 │\n",
            "├────────────┼────────┤\n",
            "│ vali       │ 0.5849 │\n",
            "├────────────┼────────┤\n",
            "│ test       │ 0.9267 │\n",
            "╘════════════╧════════╛\n",
            "Last improvement of combined validation loss happened 5 epochs ago\n",
            "\n",
            "EARLY STOPPING due to lack of validation improvement, it has been 5 epochs since last validation improvement\n",
            "\n",
            "Best validation model epoch: 3\n",
            "Best validation model loss on validation set combined: 0.3368726372718811\n",
            "Best validation model loss on test set combined: 0.46659523248672485\n",
            "\n",
            "Finished: experiment_run\n",
            "Saved to: results/experiment_run_2\n"
          ]
        }
      ],
      "source": [
        "!ludwig train --dataset \"/content/gdrive/MyDrive/Yin/climate/train/alaska_climate_change_lables.tsv\" --config \"/content/gdrive/MyDrive/Yin/climate/model/bert.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeJ7XZo2cttR"
      },
      "outputs": [],
      "source": [
        "!ludwig visualize --visualization learning_curves \\\n",
        "  --output_feature_name Survived \\\n",
        "  --training_statistics results/experiment_run_1/model/training_statistics.json \\\n",
        "       results/titanic_Model2_0/training_statistics.json \\\n",
        "  --model_names Model1 Model2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C10VCcCY-1FU",
        "outputId": "cecccd25-736b-48af-9693-1fe5b2afcbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.4.1 - Predict\n",
            "\n",
            "Dataset path: /content/gdrive/MyDrive/Yin/climate/data/alaska_2014_sub.tsv\n",
            "Model path: results/experiment_run_2/model\n",
            "\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Loading metadata from: results/experiment_run_2/model/training_set_metadata.json\n",
            "Prediction:   0% 0/132 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction:  99% 131/132 [00:49<00:00,  3.01it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction: 100% 132/132 [00:50<00:00,  2.59it/s]\n",
            "Saved to: 2014\n",
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.4.1 - Predict\n",
            "\n",
            "Dataset path: /content/gdrive/MyDrive/Yin/climate/data/alaska_2015_sub.tsv\n",
            "Model path: results/experiment_run_2/model\n",
            "\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Loading metadata from: results/experiment_run_2/model/training_set_metadata.json\n",
            "Prediction:   0% 0/52 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction:  98% 51/52 [00:22<00:00,  3.02it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction: 100% 52/52 [00:24<00:00,  2.14it/s]\n",
            "Saved to: 2015\n",
            "INFO:ludwig.api:Saved to: 2015\n",
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.4.1 - Predict\n",
            "\n",
            "Dataset path: /content/gdrive/MyDrive/Yin/climate/data/alaska_2016_sub.tsv\n",
            "Model path: results/experiment_run_2/model\n",
            "\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Loading metadata from: results/experiment_run_2/model/training_set_metadata.json\n",
            "Prediction:   0% 0/43 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction:  98% 42/43 [00:21<00:00,  3.02it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction: 100% 43/43 [00:22<00:00,  1.89it/s]\n",
            "Saved to: 2016\n",
            "INFO:ludwig.api:Saved to: 2016\n",
            "███████████████████████\n",
            "█ █ █ █  ▜█ █ █ █ █   █\n",
            "█ █ █ █ █ █ █ █ █ █ ███\n",
            "█ █   █ █ █ █ █ █ █ ▌ █\n",
            "█ █████ █ █ █ █ █ █ █ █\n",
            "█     █  ▟█     █ █   █\n",
            "███████████████████████\n",
            "ludwig v0.4.1 - Predict\n",
            "\n",
            "Dataset path: /content/gdrive/MyDrive/Yin/climate/data/alaska_2017_sub.tsv\n",
            "Model path: results/experiment_run_2/model\n",
            "\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Loading metadata from: results/experiment_run_2/model/training_set_metadata.json\n",
            "Prediction:   0% 0/40 [00:00<?, ?it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction:  98% 39/40 [00:18<00:00,  3.02it/s]The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Prediction: 100% 40/40 [00:20<00:00,  1.97it/s]\n",
            "Saved to: 2017\n",
            "INFO:ludwig.api:Saved to: 2017\n"
          ]
        }
      ],
      "source": [
        "!ludwig predict --dataset /content/gdrive/MyDrive/Yin/climate/data/alaska_2014_sub.tsv --model_path results/experiment_run_2/model --output_directory 2014\n",
        "!ludwig predict --dataset /content/gdrive/MyDrive/Yin/climate/data/alaska_2015_sub.tsv --model_path results/experiment_run_2/model --output_directory 2015\n",
        "!ludwig predict --dataset /content/gdrive/MyDrive/Yin/climate/data/alaska_2016_sub.tsv --model_path results/experiment_run_2/model --output_directory 2016\n",
        "!ludwig predict --dataset /content/gdrive/MyDrive/Yin/climate/data/alaska_2017_sub.tsv --model_path results/experiment_run_2/model --output_directory 2017"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}